{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to sample video clips\n\nIn this example, we'll learn how to sample video :term:`clips` from a video. A\nclip generally denotes a sequence or batch of frames, and is typically passed as\ninput to video models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, a bit of boilerplate: we'll download a video from the web, and define a\nplotting utility. You can ignore that part and jump right below to\n`sampling_tuto_start`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Optional\nimport torch\nimport requests\n\n\n# Video source: https://www.pexels.com/video/dog-eating-854132/\n# License: CC0. Author: Coverr.\nurl = \"https://videos.pexels.com/video-files/854132/854132-sd_640_360_25fps.mp4\"\nresponse = requests.get(url, headers={\"User-Agent\": \"\"})\nif response.status_code != 200:\n    raise RuntimeError(f\"Failed to download video. {response.status_code = }.\")\n\nraw_video_bytes = response.content\n\n\ndef plot(frames: torch.Tensor, title : Optional[str] = None):\n    try:\n        from torchvision.utils import make_grid\n        from torchvision.transforms.v2.functional import to_pil_image\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Cannot plot, please run `pip install torchvision matplotlib`\")\n        return\n\n    plt.rcParams[\"savefig.bbox\"] = 'tight'\n    fig, ax = plt.subplots()\n    ax.imshow(to_pil_image(make_grid(frames)))\n    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n    if title is not None:\n        ax.set_title(title)\n    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Creating a decoder\n\nSampling clips from a video always starts by creating a\n:class:`~torchcodec.decoders.VideoDecoder` object. If you're not already\nfamiliar with :class:`~torchcodec.decoders.VideoDecoder`, take a quick look\nat: `sphx_glr_generated_examples_basic_example.py`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchcodec.decoders import VideoDecoder\n\n# You can also pass a path to a local file!\ndecoder = VideoDecoder(raw_video_bytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling basics\n\nWe can now use our decoder to sample clips. Let's first look at a simple\nexample: all other samplers follow similar APIs and principles. We'll use\n:func:`~torchcodec.samplers.clips_at_random_indices` to sample clips that\nstart at random indices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchcodec.samplers import clips_at_random_indices\n\n# The samplers RNG is controlled by pytorch's RNG. We set a seed for this\n# tutorial to be reproducible across runs, but note that hard-coding a seed for\n# a training run is generally not recommended.\ntorch.manual_seed(0)\n\nclips = clips_at_random_indices(\n    decoder,\n    num_clips=5,\n    num_frames_per_clip=4,\n    num_indices_between_frames=3,\n)\nclips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of the sampler is a sequence of clips, represented as\n:class:`~torchcodec.FrameBatch` object. In this object, we have different\nfields:\n\n- ``data``: a 5D uint8 tensor representing the frame data. Its shape is\n  (num_clips, num_frames_per_clip, ...) where ... is either (C, H, W) or (H,\n  W, C), depending on the ``dimension_order`` parameter of the\n  :class:`~torchcodec.decoders.VideoDecoder`. This is typically what would get\n  passed to the model.\n- ``pts_seconds``: a 2D float tensor of shape (num_clips, num_frames_per_clip)\n  giving the starting timestamps of each frame within each clip, in seconds.\n- ``duration_seconds``: a 2D float tensor of shape (num_clips,\n  num_frames_per_clip) giving the duration of each frame within each clip, in\n  seconds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot(clips[0].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexing and manipulating clips\n\nClips are :class:`~torchcodec.FrameBatch` objects, and they support native\npytorch indexing semantics (including fancy indexing). This makes it easy to\nfilter clips based on a given criteria. For example, from the clips above we\ncan easily filter out those who start *after* a specific timestamp:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clip_starts = clips.pts_seconds[:, 0]\nclip_starts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clips_starting_after_five_seconds = clips[clip_starts > 5]\nclips_starting_after_five_seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "every_other_clip = clips[::2]\nevery_other_clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>A more natural and efficient way to get clips after a given timestamp is to\n  rely on the sampling range parameters, which we'll cover later in `sampling_range`.</p></div>\n\n## Index-based and Time-based samplers\n\nSo far we've used  :func:`~torchcodec.samplers.clips_at_random_indices`.\nTorchcodec support additional samplers, which fall under two main categories:\n\nIndex-based samplers:\n\n-  :func:`~torchcodec.samplers.clips_at_random_indices`\n-  :func:`~torchcodec.samplers.clips_at_regular_indices`\n\nTime-based samplers:\n\n-  :func:`~torchcodec.samplers.clips_at_random_timestamps`\n-  :func:`~torchcodec.samplers.clips_at_regular_timestamps`\n\nAll these samplers follow similar APIs and the time-based samplers have\nanalogous parameters to the index-based ones. Both samplers types generally\noffer comparable performance in terms speed.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Is it better to use a time-based sampler or an index-based sampler? The\n  index-based samplers have arguably slightly simpler APIs and their behavior\n  is possibly simpler to understand and control, because of the discrete\n  nature of indices. For videos with constant fps, an index-based sampler\n  behaves exactly the same as a time-based samplers. For videos with variable\n  fps however (as is often the case), relying on indices may under/over sample\n  some regions in the video, which may lead to undersirable side effects when\n  training a model. Using a time-based sampler ensures uniform sampling\n  caracteristics along the time-dimension.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Advanced parameters: sampling range\n\nSometimes, we may not want to sample clips from an entire video. We may only\nbe interested in clips that start within a smaller interval. In samplers, the\n``sampling_range_start`` and ``sampling_range_end`` parmeters control the\nsampling range: they define where we allow clips to *start*. There are two\nimportant things to keep in mind:\n\n- ``sampling_range_end`` is an *open* upper-bound: clips may only start within\n  [sampling_range_start, sampling_range_end).\n- Because these parameter define where a clip can start, clips may contain\n  frames *after*  ``sampling_range_end``!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchcodec.samplers import clips_at_regular_timestamps\n\nclips = clips_at_regular_timestamps(\n    decoder,\n    seconds_between_clip_starts=1,\n    num_frames_per_clip=4,\n    seconds_between_frames=0.5,\n    sampling_range_start=2,\n    sampling_range_end=5\n)\nclips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced parameters: policy\n\nDepending on the length or duration of the video and on the sampling\nparameters, the sampler may try to sample frames *beyond* the end of the\nvideo. The ``policy`` parameter defines how such invalid frames should be\nreplaced with valid\nframes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchcodec.samplers import clips_at_random_timestamps\n\nend_of_video = decoder.metadata.end_stream_seconds\nprint(f\"{end_of_video = }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nclips = clips_at_random_timestamps(\n    decoder,\n    num_clips=1,\n    num_frames_per_clip=5,\n    seconds_between_frames=0.4,\n    sampling_range_start=end_of_video - 1,\n    sampling_range_end=end_of_video,\n    policy=\"repeat_last\",\n)\nclips.pts_seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see above that the end of the video is at 13.8s. The sampler tries to\nsample frames at timestamps [13.28, 13.68, 14.08, ...] but 14.08 is an invalid\ntimestamp, beyond the end video. With the \"repeat_last\" policy, which is the\ndefault, the sampler simply repeats the last frame at 13.68 seconds to\nconstruct the clip.\n\nAn alternative policy is \"wrap\": the sampler then wraps-around the clip and repeats the first few valid frames as necessary:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nclips = clips_at_random_timestamps(\n    decoder,\n    num_clips=1,\n    num_frames_per_clip=5,\n    seconds_between_frames=0.4,\n    sampling_range_start=end_of_video - 1,\n    sampling_range_end=end_of_video,\n    policy=\"wrap\",\n)\nclips.pts_seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the value of ``sampling_range_end`` is automatically set such that\nthe sampler *doesn't* try to sample frames beyond the end of the video: the\ndefault value ensures that clips start early enough before the end. This means\nthat by default, the policy parameter rarely comes into action, and most users\nprobably don't need to worry too much about it.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}