{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. meta::\n   :description: Learn how to apply transforms during video decoding for improved memory efficiency and performance.\n\n# Decoder Transforms: Applying transforms during decoding\n\nIn this example, we will demonstrate how to use the ``transforms`` parameter of\nthe :class:`~torchcodec.decoders.VideoDecoder` class. This parameter allows us\nto specify a list of :class:`torchcodec.transforms.DecoderTransform` or\n:class:`torchvision.transforms.v2.Transform` objects. These objects serve as\ntransform specifications that the :class:`~torchcodec.decoders.VideoDecoder`\nwill apply during the decoding process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, a bit of boilerplate, definitions that we will use later. You can skip\nahead to our `example_video` or `applying_transforms`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport requests\nimport tempfile\nfrom pathlib import Path\nimport shutil\nfrom time import perf_counter_ns\n\n\ndef store_video_to(url: str, local_video_path: Path):\n    response = requests.get(url, headers={\"User-Agent\": \"\"})\n    if response.status_code != 200:\n        raise RuntimeError(f\"Failed to download video. {response.status_code = }.\")\n\n    with open(local_video_path, 'wb') as f:\n        for chunk in response.iter_content():\n            f.write(chunk)\n\n\ndef plot(frames: torch.Tensor, title : str | None = None):\n    try:\n        from torchvision.utils import make_grid\n        from torchvision.transforms.v2.functional import to_pil_image\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Cannot plot, please run `pip install torchvision matplotlib`\")\n        return\n\n    plt.rcParams[\"savefig.bbox\"] = \"tight\"\n    dpi = 300\n    fig, ax = plt.subplots(figsize=(800 / dpi, 600 / dpi), dpi=dpi)\n    ax.imshow(to_pil_image(make_grid(frames)))\n    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n    if title is not None:\n        ax.set_title(title, fontsize=6)\n    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Our example video\n\nWe'll download a video from the internet and store it locally. We're\npurposefully retrieving a high resolution video to demonstrate using\ntransforms to reduce the dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Video source: https://www.pexels.com/video/an-african-penguin-at-the-beach-9140346/\n# Author: Taryn Elliott.\nurl = \"https://videos.pexels.com/video-files/9140346/9140346-uhd_3840_2160_25fps.mp4\"\n\ntemp_dir = tempfile.mkdtemp()\npenguin_video_path = Path(temp_dir) / \"penguin.mp4\"\nstore_video_to(url, penguin_video_path)\n\nfrom torchcodec.decoders import VideoDecoder\nprint(f\"Penguin video metadata: {VideoDecoder(penguin_video_path).metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown above, the video is 37 seconds long and has a height of 2160 pixels\nand a width of 3840 pixels.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The colloquial way to report the dimensions of this video would be as\n    3840x2160; that is, (`width`, `height`). In the PyTorch ecosystem, image\n    dimensions are typically expressed as (`height`, `width`). The remainder\n    of this tutorial uses the PyTorch convention of (`height`, `width`) to\n    specify image dimensions.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Applying transforms during pre-processing\n\nA pre-processing pipeline for videos during training will typically apply a\nset of transforms for a variety of reasons. Below is a simple example of\napplying TorchVision's :class:`~torchvision.transforms.v2.Resize` transform to a single\nframe **after** the decoder returns it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2\n\nfull_decoder = VideoDecoder(penguin_video_path)\nframe = full_decoder[5]\nresized_after = v2.Resize(size=(480, 640))(frame)\n\nplot(resized_after, title=\"Resized to 480x640 after decoding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the example above, ``full_decoder`` returns a video frame that has the\ndimensions (2160, 3840) which is then resized down to (480, 640). But with the\n``transforms`` parameter of :class:`~torchcodec.decoders.VideoDecoder` we can\nspecify for the resize to  happen **during** decoding!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resize_decoder = VideoDecoder(\n    penguin_video_path,\n    transforms=[v2.Resize(size=(480, 640))]\n)\nresized_during = resize_decoder[5]\n\nplot(resized_during, title=\"Resized to 480x640 during decoding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TorchCodec's relationship to TorchVision transforms\nNotably, in our examples we are passing in TorchVision\n:class:`~torchvision.transforms.v2.Transform` objects as our transforms.\nHowever, :class:`~torchcodec.decoders.VideoDecoder` accepts TorchVision\ntransforms as a matter of convenience. TorchVision is **not required** to use\ndecoder transforms.\n\nEvery TorchVision transform that :class:`~torchcodec.decoders.VideoDecoder` accepts\nhas a complementary transform defined in :mod:`torchcodec.transforms`. We\nwould have gotten equivalent behavior if we had passed in the\n:class:`torchcodec.transforms.Resize` object that is a part of TorchCodec.\n:class:`~torchcodec.decoders.VideoDecoder` accepts both objects as a matter of\nconvenience and to clarify the relationship between the transforms that TorchCodec\napplies and the transforms that TorchVision offers.\n\nImportantly, the two frames are not identical, even though we can see they\n*look* very similar:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "abs_diff = (resized_after.float() - resized_during.float()).abs()\n(abs_diff == 0).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But they're close enough that models won't be able to tell a difference:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert (abs_diff <= 1).float().mean() >= 0.998"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While :class:`~torchcodec.decoders.VideoDecoder` accepts TorchVision transforms as\n*specifications*, it is not actually using the TorchVision implementation of these\ntransforms. Instead, it is mapping them to equivalent\n[FFmpeg filters](https://ffmpeg.org/ffmpeg-filters.html). That is,\n:class:`torchvision.transforms.v2.Resize` and :class:`torchcodec.transforms.Resize` are mapped to\n[scale](https://ffmpeg.org/ffmpeg-filters.html#scale-1); and\n:class:`torchvision.transforms.v2.CenterCrop` and :class:`torchcodec.transforms.CenterCrop` are mapped to\n[crop](https://ffmpeg.org/ffmpeg-filters.html#crop).\n\nThe relationships we ensure between TorchCodec :class:`~torchcodec.transforms.DecoderTransform` objects\nand TorchVision :class:`~torchvision.transforms.v2.Transform` objects are:\n\n     1. The names are the same.\n     2. Default behaviors are the same.\n     3. The parameters for the :class:`~torchcodec.transforms.DecoderTransform`\n        object are a subset of the TorchVision :class:`~torchvision.transforms.v2.Transform`\n        object.\n     4. Parameters with the same name control the same behavior and accept a\n        subset of the same types.\n     5. The difference between the frames returned by a decoder transform and\n        the complementary TorchVision transform are such that a model should\n        not be able to tell the difference.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Applying the exact same transforms during training and inference is\n    important for model perforamnce. For example, if you use decoder\n    transforms to resize frames during training, you should also use decoder\n    transforms to resize frames during inference. We provide the similarity\n    guarantees to mitigate the harm when the two techniques are\n    *unintentionally* mixed. That is, if you use decoder transforms to resize\n    frames during training, but use TorchVisions's\n    :class:`~torchvision.transforms.v2.Resize` during inference, our guarantees\n    mitigate the harm to model performance. But we **reccommend against** this kind of\n    mixing.\n\n    It is appropriate and expected to use some decoder transforms and some TorchVision\n    transforms, as long as the exact same pre-processing operations are performed during\n    training and inference.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoder transform pipelines\nSo far, we've only provided a single transform to the ``transform`` parameter to\n:class:`~torchcodec.decoders.VideoDecoder`. But it\nactually accepts a list of transforms, which become a pipeline of transforms.\nThe order of the list matters: the first transform in the list will receive\nthe originally decoded frame. The output of that transform becomes the input\nto the next transform in the list, and so on.\n\nA simple example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crop_resize_decoder = VideoDecoder(\n    penguin_video_path,\n    transforms = [\n        v2.CenterCrop(size=(1280, 1664)),\n        v2.Resize(size=(480, 640)),\n    ]\n)\ncrop_resized_during = crop_resize_decoder[5]\nplot(crop_resized_during, title=\"Center cropped then resized to 480x640\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance: memory efficiency and speed\n\nThe main motivation for decoder transforms is *memory efficiency*,\nparticularly when applying transforms that reduce the size of a frame, such\nas resize and crop. Because the FFmpeg layer knows all of the transforms it\nneeds to apply during decoding, it's able to efficiently reuse memory.\nFurther, full resolution frames are never returned to the Python layer.  As a\nresult, there is significantly less total memory needed and less pressure on\nthe Python garbage collector.\n\nIn [benchmarks](https://github.com/meta-pytorch/torchcodec/blob/f6a816190cbcac417338c29d5e6fac99311d054f/benchmarks/decoders/benchmark_transforms.py)\nreducing frames from (1080, 1920) down to (135, 240), we have observed a\nreduction in peak resident set size from 4.3 GB to 0.4 GB.\n\nThere is sometimes a runtime benefit, but it is dependent on the number of\nthreads that the :class:`~torchcodec.decoders.VideoDecoder` tells FFmpeg\nto use. We define the following benchmark function, as well as the functions\nto benchmark:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bench(f, average_over=3, warmup=1, **f_kwargs):\n    for _ in range(warmup):\n        f(**f_kwargs)\n\n    times = []\n    for _ in range(average_over):\n        start_time = perf_counter_ns()\n        f(**f_kwargs)\n        end_time = perf_counter_ns()\n        times.append(end_time - start_time)\n\n    times = torch.tensor(times) * 1e-6  # ns to ms\n    times_std = times.std().item()\n    times_med = times.median().item()\n    return f\"{times_med = :.2f}ms +- {times_std:.2f}\"\n\n\nfrom torchcodec import samplers\n\n\ndef sample_decoder_transforms(num_threads: int):\n    decoder = VideoDecoder(\n        penguin_video_path,\n        transforms = [\n            v2.CenterCrop(size=(1280, 1664)),\n            v2.Resize(size=(480, 640)),\n        ],\n        seek_mode=\"approximate\",\n        num_ffmpeg_threads=num_threads,\n    )\n    transformed_frames = samplers.clips_at_regular_indices(\n        decoder,\n        num_clips=1,\n        num_frames_per_clip=200\n    )\n    assert len(transformed_frames.data[0]) == 200\n\n\ndef sample_torchvision_transforms(num_threads: int):\n    if num_threads > 0:\n        torch.set_num_threads(num_threads)\n    decoder = VideoDecoder(\n        penguin_video_path,\n        seek_mode=\"approximate\",\n        num_ffmpeg_threads=num_threads,\n    )\n    frames = samplers.clips_at_regular_indices(\n        decoder,\n        num_clips=1,\n        num_frames_per_clip=200\n    )\n    transforms = v2.Compose(\n        [\n            v2.CenterCrop(size=(1280, 1664)),\n            v2.Resize(size=(480, 640)),\n        ]\n    )\n    transformed_frames = transforms(frames.data)\n    assert transformed_frames.shape[1] == 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the :class:`~torchcodec.decoders.VideoDecoder` object sets the number of\nFFmpeg threads to 0, that tells FFmpeg to determine how many threads to use\nbased on what is available on the current system. In such cases, decoder transforms\nwill tend to outperform getting back a full frame and applying TorchVision transforms\nsequentially:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"decoder transforms:    {bench(sample_decoder_transforms, num_threads=0)}\")\nprint(f\"torchvision transform: {bench(sample_torchvision_transforms, num_threads=0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reason is that FFmpeg is applying the decoder transforms in parallel.\nHowever, if the number of threads is 1 (as is the default), then there is often\nless benefit to using decoder transforms. Using the TorchVision transforms may\neven be faster!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"decoder transforms:    {bench(sample_decoder_transforms, num_threads=1)}\")\nprint(f\"torchvision transform: {bench(sample_torchvision_transforms, num_threads=1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In brief, our performance guidance is:\n\n   1. If you are applying a transform pipeline that signficantly reduces\n      the dimensions of your input frames and memory efficiency matters, use\n      decoder transforms.\n   2. If you are using multiple FFmpeg threads, decoder transforms may be\n      faster. Experiment with your setup to verify.\n   3. If you are using a single FFmpeg thread, then decoder transforms may\n      be slower. Experiment with your setup to verify.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(temp_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}